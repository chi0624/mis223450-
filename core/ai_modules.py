import os
import re
import json
import tempfile
from typing import List, Dict, Any, Optional

from dotenv import load_dotenv
from openai import OpenAI
from django.conf import settings
from .models import Lecture, Question

# éŸ³è¨Šè™•ç†ï¼ˆéœ€ ffmpegï¼›ä½ çš„ apt.txt å·²å« ffmpegï¼‰
from pydub import AudioSegment

load_dotenv()  # æœ¬åœ°è®€ .envï¼›Render è®€ Environment


# =========================
# OpenAI Client
# =========================
def create_openai_client() -> OpenAI:
    api_key = os.getenv("OPENAI_API_KEY") or getattr(settings, "OPENAI_API_KEY", None)
    api_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
    if not api_key or api_key.strip().upper() == "EMPTY":
        raise ValueError("âŒ è«‹è¨­å®š OPENAI_API_KEYï¼ˆRender â†’ Environment æˆ–æœ¬åœ° .envï¼‰")
    return OpenAI(api_key=api_key, base_url=api_base)


# =========================
# é•·éŸ³æª”ï¼šè½‰æ ¼å¼ + åˆ‡ç‰‡ï¼ˆ8 åˆ†é˜ä¸€æ®µï¼‰
# =========================
def _prepare_audio_chunks(src_path: str,
                          chunk_ms: int = 8 * 60 * 1000,
                          overlap_ms: int = 2000) -> List[str]:
    """
    å°‡ä»»æ„é•·åº¦éŸ³æª”è½‰ç‚º 16kHz mono wavï¼Œåˆ‡æ®µï¼Œå›å‚³è‡¨æ™‚æª”è·¯å¾‘æ¸…å–®ã€‚
    éœ€ç³»çµ±æœ‰ ffmpegï¼ˆç”± apt.txt å®‰è£ï¼‰ã€‚
    """
    if not os.path.exists(src_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°éŸ³æª”ï¼š{src_path}")

    audio = AudioSegment.from_file(src_path)
    audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)

    paths: List[str] = []
    duration = len(audio)
    start = 0
    idx = 0

    while start < duration:
        end = min(start + chunk_ms, duration)
        segment = audio[max(0, start - overlap_ms):end]
        fd, tmp_path = tempfile.mkstemp(suffix=f".chunk{idx}.wav")
        os.close(fd)
        segment.export(tmp_path, format="wav")
        paths.append(tmp_path)
        start = end
        idx += 1

    return paths


def transcribe_with_whisper(audio_path: str) -> Optional[str]:
    """
    é‡å°é•·éŸ³æª”ï¼šå…ˆåˆ‡ç‰‡å¾Œé€æ®µç”¨ whisper-1 è½‰éŒ„ï¼Œæœ€å¾Œåˆä½µã€‚
    """
    try:
        if not os.path.exists(audio_path):
            print(f"âŒ æ‰¾ä¸åˆ°éŸ³è¨Šæª”æ¡ˆï¼š{audio_path}")
            return None

        print("âœ… Whisper API è½‰éŒ„é–‹å§‹ï¼ˆé•·éŸ³æª”è‡ªå‹•åˆ†æ®µï¼‰")
        client = create_openai_client()
        chunk_paths = _prepare_audio_chunks(audio_path)

        pieces: List[str] = []
        for i, cpath in enumerate(chunk_paths):
            try:
                with open(cpath, "rb") as f:
                    resp = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=f
                    )
                text = getattr(resp, "text", None) or (resp if isinstance(resp, str) else "")
                print(f"  â””â”€ åˆ†æ®µ {i+1}/{len(chunk_paths)} å®Œæˆï¼Œé•·åº¦ï¼š{len(text)}")
                pieces.append((text or "").strip())
            finally:
                try:
                    os.remove(cpath)
                except Exception:
                    pass

        full_text = "\n".join(t for t in pieces if t)
        return full_text.strip() or None

    except Exception as e:
        print(f"âŒ Whisper API è½‰éŒ„éŒ¯èª¤: {e}")
        return None


# =========================
# æ–‡æœ¬åˆ†æ®µï¼ˆé¿å… prompt éé•·ï¼‰
# =========================
def dynamic_split(text: str, min_length: int = 300, max_length: int = 1000) -> List[str]:
    text = text.strip()
    if len(text) <= max_length:
        return [text]
    paragraphs = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])\s*', text)
    chunks, temp = [], ""
    for para in paragraphs:
        if len(temp) + len(para) <= max_length:
            temp += para
        else:
            if len(temp) >= min_length:
                chunks.append(temp.strip())
                temp = para
            else:
                temp += para
    if temp:
        chunks.append(temp.strip())
    return chunks


# =========================
# æ‘˜è¦ï¼ˆåˆ†æ®µ + ç¸½çµï¼‰
# =========================
def generate_summary_for_chunk(client: OpenAI, chunk: str, chunk_index: int, total_chunks: int) -> str:
    messages = [
        {"role": "system", "content": f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡èª²ç¨‹æ‘˜è¦è¨­è¨ˆå¸«ã€‚
è«‹é‡å°ç¬¬ {chunk_index + 1} æ®µèª²ç¨‹å…§å®¹é€²è¡Œé‡é»æ‘˜è¦ï¼ŒåŒ…å«ï¼š
- ç°¡æ½”å…§å®¹æ¦‚è¿°ï¼ˆ50â€“80å­—ï¼‰
- 2â€“4 å€‹å­¸ç¿’è¦é»ï¼Œä½¿ç”¨æ¢åˆ—å¼
ç¸½å­—æ•¸æ§åˆ¶åœ¨ 150 å­—å…§ã€‚"""},
        {"role": "user", "content": chunk}
    ]
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",   # è¼ƒå¿«ã€è¼ƒä¸æ˜“è¶…æ™‚
            messages=messages,
            temperature=0.3,
            max_tokens=400
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ æ®µè½æ‘˜è¦éŒ¯èª¤: {e}")
        return f"ç¬¬ {chunk_index + 1} æ®µæ‘˜è¦å¤±æ•—"


def combine_summaries(client: OpenAI, summaries: List[str]) -> str:
    combined = "\n\n".join([f"æ®µè½ {i+1}ï¼š{s}" for i, s in enumerate(summaries)])
    messages = [
        {"role": "system", "content": """ä½ æ˜¯æ•™è‚²è¨­è¨ˆå°ˆå®¶ï¼Œè«‹å°‡ä¸‹åˆ—åˆ†æ®µæ‘˜è¦çµ±æ•´ç‚ºå®Œæ•´èª²ç¨‹æ‘˜è¦ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

ã€èª²ç¨‹æ¦‚è¿°ã€‘ï¼šèªªæ˜æ•´é«”èª²ç¨‹å…§å®¹èˆ‡é‡è¦æ€§ï¼ˆ150â€“200å­—ï¼‰
ã€å­¸ç¿’é‡é»ã€‘ï¼šåˆ—å‡ºæœ¬èª²ç¨‹çš„ 4â€“5 å€‹å­¸ç¿’ç›®æ¨™ï¼ˆæ¢åˆ—ï¼‰
ã€å®Œæˆå¾Œæ”¶ç©«ã€‘ï¼šç°¡è¿°å­¸ç”Ÿå®Œæˆèª²ç¨‹å¾Œèƒ½å…·å‚™çš„èƒ½åŠ›ï¼ˆ60å­—å…§ï¼‰

è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œé¿å…é‡è¤‡æ•˜è¿°ï¼Œæ§åˆ¶ç¸½å­—æ•¸åœ¨ 400 å­—å…§ã€‚"""},
        {"role": "user", "content": combined}
    ]
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.3,
            max_tokens=512
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ ç¸½çµéŒ¯èª¤: {e}")
        return "æ•´åˆæ‘˜è¦å¤±æ•—"


# =========================
# å‡ºé¡Œï¼ˆé¸æ“‡é¡Œ + æ˜¯éé¡Œï¼‰
# =========================
def normalize_mcq_payload(data: Any) -> List[Dict[str, Any]]:
    if isinstance(data, dict) and "items" in data and isinstance(data["items"], list):
        items = data["items"]
    elif isinstance(data, list):
        items = data
    else:
        raise ValueError("JSON çµæ§‹ä¸ç¬¦åˆé æœŸ")

    cleaned: List[Dict[str, Any]] = []
    for it in items:
        try:
            concept = str(it["concept"]).strip()
            question = str(it["question"]).strip()
            options = it["options"]
            answer = str(it["answer"]).strip().upper()
            explanation = str(it.get("explanation", "")).strip()
            if not isinstance(options, dict):
                continue
            if not all(k in options for k in ["A", "B", "C", "D"]):
                continue
            if answer not in {"A", "B", "C", "D"}:
                continue
            cleaned.append({
                "concept": concept,
                "question": question,
                "options": {
                    "A": str(options["A"]),
                    "B": str(options["B"]),
                    "C": str(options["C"]),
                    "D": str(options["D"]),
                },
                "answer": answer,
                "explanation": explanation,
            })
        except Exception:
            continue
    return cleaned


def safe_json_parse(raw: str) -> List[Dict[str, Any]]:
    try:
        return normalize_mcq_payload(json.loads(raw))
    except Exception:
        pass
    m = re.search(r'```json\s*([\s\S]*?)```', raw, re.IGNORECASE)
    if not m:
        m = re.search(r'(\{[\s\S]*\}|\[[\s\S]*\])', raw)
    if m:
        candidate = m.group(1)
        candidate = candidate.replace("â€œ", '"').replace("â€", '"').replace("â€™", "'")
        candidate = re.sub(r",\s*([\]}])", r"\1", candidate)
        try:
            return normalize_mcq_payload(json.loads(candidate))
        except Exception as e:
            print("âŒ JSON å€å¡Šè§£æä»å¤±æ•—ï¼š", e)
    print("âš ï¸ MCQ åŸå§‹å›æ‡‰ï¼ˆæˆªæ–· 500 å­—ï¼‰ï¼š", raw[:500])
    raise ValueError("æ¨¡å‹æœªå›å‚³åˆæ³• JSON")


def generate_quiz_with_retry(client: OpenAI, summary: str, count: int = 3) -> List[Dict[str, Any]]:
    system = f"""ä½ æ˜¯ä¸€ä½èª²ç¨‹å‡ºé¡Œ AIï¼Œè«‹æ ¹æ“šä»¥ä¸‹èª²ç¨‹æ‘˜è¦ç”¢ç”Ÿ {count} é¡Œé¸æ“‡é¡Œã€‚
åš´æ ¼ä¸”åªè¼¸å‡º JSONã€Œé™£åˆ—ã€ï¼Œä¸è¦ä»»ä½•èªªæ˜ã€ä¸è¦åŠ  ```jsonã€‚æ¯ä¸€é¡Œç‰©ä»¶å¿…é ˆåŒ…å«ï¼š
concept, question, options(A/B/C/D), answer(åªèƒ½æ˜¯ A/B/C/D), explanationã€‚"""
    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": summary}
    ]
    # å˜—è©¦ 1
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.2,
            max_tokens=1500
        )
        raw = resp.choices[0].message.content or ""
        return safe_json_parse(raw)
    except Exception as e:
        print("ğŸ” ç¬¬ä¸€æ¬¡è§£æå¤±æ•—ï¼Œæ”¹ç”¨æ›´åš´æ ¼æç¤ºé‡è©¦ï¼š", e)

    # å˜—è©¦ 2ï¼ˆæ›´åš´æ ¼ï¼‰
    hard_prompt = summary + f"\n\nè«‹åš´æ ¼è¼¸å‡º {count} é¡Œé¸æ“‡é¡Œï¼Œåªè¼¸å‡º JSON é™£åˆ—ï¼Œæ ¼å¼åš´è¬¹ã€‚ä¸è¦æœ‰ä»»ä½•èªªæ˜ã€```jsonã€å‰å¾Œæ–‡å­—ã€‚"
    try:
        messages = [
            {"role": "system", "content": "ä½ åªæœƒè¼¸å‡º JSONã€‚"},
            {"role": "user", "content": hard_prompt}
        ]
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.1,
            max_tokens=1500
        )
        raw = resp.choices[0].message.content or ""
        return safe_json_parse(raw)
    except Exception as e:
        print("âŒ é‡è©¦ä»å¤±æ•—ï¼š", e)
        return []


def generate_tf_questions(client: OpenAI, summary: str, count: int) -> List[Dict[str, Any]]:
    messages = [
        {
            "role": "system",
            "content": f"""è«‹æ ¹æ“šä»¥ä¸‹èª²ç¨‹æ‘˜è¦ï¼Œè¨­è¨ˆ {count} é¡Œæ˜¯éé¡Œï¼ˆTrue/Falseï¼‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
  {{
    "concept": "å­¸ç¿’æ¦‚å¿µ",
    "question": "å•é¡Œå…§å®¹",
    "answer": "True",
    "explanation": "æ­£ç¢ºç­”æ¡ˆè§£æ"
  }},
  ...
]
è«‹å›å‚³ JSON æ ¼å¼è³‡æ–™ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—èªªæ˜ã€‚"""
        },
        {"role": "user", "content": summary}
    ]
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.3,
            max_tokens=1200
        )
        return json.loads(resp.choices[0].message.content)
    except Exception as e:
        print(f"âŒ æ˜¯éé¡Œç”¢ç”Ÿå¤±æ•—ï¼š{e}")
        return []


# =========================
# å¯«å…¥è³‡æ–™åº«
# =========================
def parse_and_store_questions(summary: str, quiz_data: List[Dict[str, Any]], lecture: Lecture, question_type: str) -> None:
    if not quiz_data:
        return
    for item in quiz_data:
        try:
            if question_type == 'mcq':
                options = item.get('options', {}) or {}
                Question.objects.create(
                    lecture=lecture,
                    question_text=str(item.get('question', '')).strip(),
                    option_a=str(options.get('A', '')).strip(),
                    option_b=str(options.get('B', '')).strip(),
                    option_c=str(options.get('C', '')).strip(),
                    option_d=str(options.get('D', '')).strip(),
                    correct_answer=str(item.get('answer', '')).strip(),
                    explanation=str(item.get('explanation', '')).strip(),
                    question_type='mcq'
                )
            elif question_type == 'tf':
                Question.objects.create(
                    lecture=lecture,
                    question_text=str(item.get('question', '')).strip(),
                    correct_answer=str(item.get('answer', '')).strip(),
                    explanation=str(item.get('explanation', '')).strip(),
                    question_type='tf'
                )
        except Exception as e:
            print(f"âš ï¸ å„²å­˜é¡Œç›®å¤±æ•—ï¼š{e}")


# =========================
# Pipelineï¼šéŸ³æª” â†’ æ‘˜è¦ â†’ é¡Œåº«
# =========================
def process_audio_and_generate_quiz(lecture_id: int, num_mcq: int = 3, num_tf: int = 0) -> None:
    lecture = Lecture.objects.get(id=lecture_id)
    client = create_openai_client()

    print("ğŸ§ é–‹å§‹èªéŸ³è½‰éŒ„")
    transcript = transcribe_with_whisper(lecture.audio_file.path)
    if not transcript:
        print("âŒ è½‰éŒ„å¤±æ•—ï¼Œæµç¨‹çµ‚æ­¢")
        return
    lecture.transcript = transcript
    lecture.save()

    print("ğŸ“ é–‹å§‹æ‘˜è¦è™•ç†")
    chunks = dynamic_split(transcript)
    summaries = [generate_summary_for_chunk(client, c, i, len(chunks)) for i, c in enumerate(chunks)]
    final_summary = combine_summaries(client, summaries)
    lecture.summary = final_summary
    lecture.save()

    print("ğŸ§  é–‹å§‹ç”¢ç”Ÿè€ƒé¡Œ")
    if num_mcq > 0:
        mcq_data = generate_quiz_with_retry(client, final_summary, num_mcq)
        if mcq_data:
            parse_and_store_questions(final_summary, mcq_data, lecture, 'mcq')
        else:
            print("âš ï¸ æ²’æœ‰å›å‚³ MCQ é¡Œç›®")

    if num_tf > 0:
        tf_data = generate_tf_questions(client, final_summary, num_tf)
        if tf_data:
            parse_and_store_questions(final_summary, tf_data, lecture, 'tf')
        else:
            print("âš ï¸ æ²’æœ‰å›å‚³ TF é¡Œç›®")


def process_transcript_and_generate_quiz(lecture: Lecture, client: Optional[OpenAI] = None,
                                         num_mcq: int = 3, num_tf: int = 0) -> None:
    client = client or create_openai_client()

    transcript = lecture.transcript
    if not transcript:
        print("âŒ ç„¡è½‰éŒ„å…§å®¹ï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦èˆ‡é¡Œç›®")
        return

    print("ğŸ“ é–‹å§‹æ‘˜è¦è™•ç†")
    chunks = dynamic_split(transcript)
    summaries = [generate_summary_for_chunk(client, c, i, len(chunks)) for i, c in enumerate(chunks)]
    final_summary = combine_summaries(client, summaries)
    lecture.summary = final_summary
    lecture.save()

    print("ğŸ§  é–‹å§‹ç”¢ç”Ÿè€ƒé¡Œ")
    if num_mcq > 0:
        mcq_data = generate_quiz_with_retry(client, final_summary, num_mcq)
        if mcq_data:
            parse_and_store_questions(final_summary, mcq_data, lecture, 'mcq')
        else:
            print("âš ï¸ æ²’æœ‰å›å‚³ MCQ é¡Œç›®")

    if num_tf > 0:
        tf_data = generate_tf_questions(client, final_summary, num_tf)
        if tf_data:
            parse_and_store_questions(final_summary, tf_data, lecture, 'tf')
        else:
            print("âš ï¸ æ²’æœ‰å›å‚³ TF é¡Œç›®")
